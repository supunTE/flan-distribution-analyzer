{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get hugging face file names\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import requests\n",
    "\n",
    "repo_id = \"Open-Orca/FLAN\"\n",
    "subfolder = \"flan_fsnoopt_data\"\n",
    "local_base_path = \"datasets\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def get_subfolder_files(repo_id, subfolder):\n",
    "    repo_files = api.list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "    return [file for file in repo_files if file.startswith(f\"{subfolder}/\")]\n",
    "\n",
    "subfolder_files = get_subfolder_files(repo_id, subfolder)\n",
    "len(subfolder_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flan_fsnoopt_data/part.0.parquet',\n",
       " 'flan_fsnoopt_data/part.18.parquet',\n",
       " 'flan_fsnoopt_data/part.36.parquet',\n",
       " 'flan_fsnoopt_data/part.54.parquet',\n",
       " 'flan_fsnoopt_data/part.72.parquet',\n",
       " 'flan_fsnoopt_data/part.91.parquet',\n",
       " 'flan_fsnoopt_data/part.109.parquet',\n",
       " 'flan_fsnoopt_data/part.127.parquet',\n",
       " 'flan_fsnoopt_data/part.145.parquet',\n",
       " 'flan_fsnoopt_data/part.164.parquet']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evenly spaced indices for download\n",
    "import numpy as np\n",
    "\n",
    "indices = np.linspace(0, len(subfolder_files)-1, 10, dtype=int)\n",
    "\n",
    "subfolder_file_prefix = subfolder_files[0].split(\".\")[0]\n",
    "selected_subfolder_files = [f\"{subfolder_file_prefix}.{i}.parquet\" for i in indices] \n",
    "selected_subfolder_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4295ae1eaa834df68e9ae7e9b104d925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.54.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2b5ffd5bcb4e0987b1bd094467489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.18.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdacc84ff0b433da39fc75384996b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.0.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e151920ccfb4b2a8dc1f5f26278ba65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.36.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c646399ffe443feb209ad096285a7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.72.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f43f8b11c0148a48083cff3e064267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.91.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc87defbeba94495ad81f1cf24310f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.109.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c606e91c474c4daa42ae2e3a3719c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.127.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0488b6a007b4e7c89c65825268e9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.145.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e762e28642a84d36a2317d4a15ef42bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.164.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download paraquet files\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def download_file(api, repo_id, file, local_base_path):\n",
    "    api.hf_hub_download(repo_id=repo_id, repo_type=\"dataset\", \n",
    "                                         filename=file, local_dir=local_base_path)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(download_file, api, repo_id, file, local_base_path) \n",
    "                   for file in selected_subfolder_files]\n",
    "        \n",
    "        concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset\n",
    "dataset_name = \"flan_fsnoopt_data\"\n",
    "folder_path = os.path.join(local_base_path, dataset_name)\n",
    "stats_folder_name = \"stats_by_dataset\"\n",
    "dataset_prefix = dataset_name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files 'datasets/flan_fsnoopt_data': 10\n"
     ]
    }
   ],
   "source": [
    "# Check all files are downloaded\n",
    "def get_parquet_files_by_folder(folder_path):\n",
    "    items = os.listdir(folder_path)\n",
    "    return [item for item in items if item.endswith('.parquet') and os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "parquet_files = get_parquet_files_by_folder(folder_path)\n",
    "print(f\"Total files '{folder_path}': {len(parquet_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part.0.parquet\n",
      "part.18.parquet\n",
      "part.36.parquet\n",
      "part.54.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part.72.parquet\n",
      "part.91.parquet\n",
      "part.109.parquet\n",
      "part.127.parquet\n",
      "part.145.parquet\n",
      "part.164.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Stats\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def append_columns_to_dict(file_path, columns):\n",
    "    df = pd.read_parquet(file_path, columns=columns)\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        counts = df[column].value_counts().to_dict()\n",
    "        counts_dict[column] = counts\n",
    "\n",
    "    return counts_dict\n",
    "\n",
    "def save_dict_to_json(data_dict, json_file_path):\n",
    "    os.makedirs(os.path.dirname(json_file_path), exist_ok=True)\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=4)\n",
    "\n",
    "columns = [\"_task_name\", \"_template_type\", \"_task_source\", \"_template_idx\"]\n",
    "\n",
    "subfolder_file_prefix = parquet_files[0].split(\".\")[0]\n",
    "\n",
    "for i in indices:\n",
    "    file = f\"{subfolder_file_prefix}.{i}.parquet\"\n",
    "    print(file)\n",
    "    data_dict = append_columns_to_dict(f\"datasets/flan_fsopt_data/{file}\", columns)\n",
    "    json_file_path = f\"{stats_folder_name}/{dataset_prefix}/{dataset_name}-{i}.json\"\n",
    "    save_dict_to_json(data_dict, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Task Distribution Sheet\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "distribution_folder = \"task_distributions\"\n",
    "\n",
    "os.makedirs(distribution_folder, exist_ok=True)\n",
    "distribution_file_path = f\"{distribution_folder}/{dataset_prefix}.csv\"\n",
    "\n",
    "\n",
    "def get_json_files_by_folder(folder_path):\n",
    "    items = os.listdir(folder_path)\n",
    "    return [item for item in items if item.endswith('.json') and os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "json_files = get_json_files_by_folder(f\"{stats_folder_name}/{dataset_prefix}\")\n",
    "\n",
    "# Sort files prefix then index\n",
    "json_files.sort(key=lambda x: x.split(\"-\")[0])\n",
    "\n",
    "tasks_data = {}\n",
    "dataset_total = {}\n",
    "\n",
    "for file_name in json_files:\n",
    "    with open(f\"{stats_folder_name}/{dataset_prefix}/{file_name}\") as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "\n",
    "        dataset_total[file_name] = sum(data_dict[\"_task_name\"].values())\n",
    "\n",
    "        for task, count in data_dict[\"_task_name\"].items():\n",
    "            if task not in tasks_data:\n",
    "                tasks_data[task] = {}\n",
    "            tasks_data[task][file_name] = count\n",
    "\n",
    "\n",
    "header = [\"Task Name\"] + [file_name.split(\".\")[0] for file_name in json_files]\n",
    "\n",
    "with open(distribution_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for task, counts in tasks_data.items():\n",
    "        total_count = sum(counts.values())\n",
    "        row = [task] + [counts.get(file_name, 0) for file_name in json_files]\n",
    "        writer.writerow(row)\n",
    "\n",
    "    writer.writerow([\"Total\"] + [dataset_total[file_name] for file_name in json_files])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Diffs\n",
    "\n",
    "import difflib\n",
    "\n",
    "def compare_files(file1, file2):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        diff = difflib.unified_diff(\n",
    "            f1.readlines(),\n",
    "            f2.readlines(),\n",
    "            fromfile=file1,\n",
    "            tofile=file2,\n",
    "        )\n",
    "        return list(diff)\n",
    "\n",
    "def compare_multiple_files(file_list):\n",
    "    comparisons = {}\n",
    "    for i in range(len(file_list)):\n",
    "        for j in range(i + 1, len(file_list)):\n",
    "            file1 = file_list[i]\n",
    "            file2 = file_list[j]\n",
    "            diff = compare_files(file1, file2)\n",
    "            comparisons[f\"{file1} vs {file2}\"] = diff\n",
    "    return comparisons\n",
    "\n",
    "def save_diffs_to_html(diffs, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"<html><head><title>File Comparisons</title></head><body>\")\n",
    "        f.write(\"<h1>File Comparisons</h1>\")\n",
    "        for comparison, diff in diffs.items():\n",
    "            f.write(f\"<h2>Comparing {comparison}</h2>\")\n",
    "            f.write(\"<pre>\")\n",
    "            for line in diff:\n",
    "                if line.startswith('+'):\n",
    "                    f.write(f'<span style=\"color: green;\">{line}</span>')\n",
    "                elif line.startswith('-'):\n",
    "                    f.write(f'<span style=\"color: red;\">{line}</span>')\n",
    "                elif line.startswith('^'):\n",
    "                    f.write(f'<span style=\"color: blue;\">{line}</span>')\n",
    "                else:\n",
    "                    f.write(line)\n",
    "            f.write(\"</pre><hr>\")\n",
    "        f.write(\"</body></html>\")\n",
    "\n",
    "file_list = [f\"{stats_folder}/{file}\" for file in os.listdir(stats_folder_name) if file.endswith('.json')]\n",
    "diffs = compare_multiple_files(file_list)\n",
    "output_file = f\"comparison_output-{dataset_name}.html\"\n",
    "save_diffs_to_html(diffs, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

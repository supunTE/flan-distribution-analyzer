{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get hugging face file names\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import requests\n",
    "\n",
    "repo_id = \"Open-Orca/FLAN\"\n",
    "subfolder = \"flan_fsnoopt_data\"\n",
    "local_base_path = \"datasets\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def get_subfolder_files(repo_id, subfolder):\n",
    "    repo_files = api.list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "    return [file for file in repo_files if file.startswith(f\"{subfolder}/\")]\n",
    "\n",
    "subfolder_files = get_subfolder_files(repo_id, subfolder)\n",
    "len(subfolder_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flan_fsnoopt_data/part.0.parquet',\n",
       " 'flan_fsnoopt_data/part.18.parquet',\n",
       " 'flan_fsnoopt_data/part.36.parquet',\n",
       " 'flan_fsnoopt_data/part.54.parquet',\n",
       " 'flan_fsnoopt_data/part.72.parquet',\n",
       " 'flan_fsnoopt_data/part.91.parquet',\n",
       " 'flan_fsnoopt_data/part.109.parquet',\n",
       " 'flan_fsnoopt_data/part.127.parquet',\n",
       " 'flan_fsnoopt_data/part.145.parquet',\n",
       " 'flan_fsnoopt_data/part.164.parquet']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evenly spaced indices for download\n",
    "import numpy as np\n",
    "\n",
    "indices = np.linspace(0, len(subfolder_files)-1, 10, dtype=int)\n",
    "\n",
    "subfolder_file_prefix = subfolder_files[0].split(\".\")[0]\n",
    "selected_subfolder_files = [f\"{subfolder_file_prefix}.{i}.parquet\" for i in indices] \n",
    "selected_subfolder_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download paraquet files\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def download_file(api, repo_id, file, local_base_path):\n",
    "    api.hf_hub_download(repo_id=repo_id, repo_type=\"dataset\", \n",
    "                                         filename=file, local_dir=local_base_path)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(download_file, api, repo_id, file, local_base_path) \n",
    "                   for file in selected_subfolder_files]\n",
    "        \n",
    "        concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset\n",
    "dataset_name = \"flan_fsnoopt_data\"\n",
    "folder_path = os.path.join(local_base_path, dataset_name)\n",
    "stats_folder_name = \"stats_by_dataset\"\n",
    "dataset_prefix = dataset_name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files 'datasets/flan_fsnoopt_data': 10\n"
     ]
    }
   ],
   "source": [
    "# Check all files are downloaded\n",
    "def get_parquet_files_by_folder(folder_path):\n",
    "    items = os.listdir(folder_path)\n",
    "    return [item for item in items if item.endswith('.parquet') and os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "parquet_files = get_parquet_files_by_folder(folder_path)\n",
    "print(f\"Total files '{folder_path}': {len(parquet_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part.0.parquet\n",
      "part.18.parquet\n",
      "part.36.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part.54.parquet\n",
      "part.72.parquet\n",
      "part.91.parquet\n",
      "part.109.parquet\n",
      "part.127.parquet\n",
      "part.145.parquet\n",
      "part.164.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Stats\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def append_columns_to_dict(file_path, columns):\n",
    "    df = pd.read_parquet(file_path, columns=columns)\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        counts = df[column].value_counts().to_dict()\n",
    "        counts_dict[column] = counts\n",
    "\n",
    "    return counts_dict\n",
    "\n",
    "def save_dict_to_json(data_dict, json_file_path):\n",
    "    os.makedirs(os.path.dirname(json_file_path), exist_ok=True)\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=4)\n",
    "\n",
    "columns = [\"_task_name\", \"_template_type\", \"_task_source\", \"_template_idx\"]\n",
    "\n",
    "subfolder_file_prefix = \"part\"\n",
    "\n",
    "for i in indices:\n",
    "    file = f\"{subfolder_file_prefix}.{i}.parquet\"\n",
    "    print(file)\n",
    "    data_dict = append_columns_to_dict(f\"datasets/{dataset_name}/{file}\", columns)\n",
    "    json_file_path = f\"{stats_folder_name}/{dataset_prefix}/{dataset_name}-{i}.json\"\n",
    "    save_dict_to_json(data_dict, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Task Distribution Sheet\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "distribution_folder = \"task_distributions\"\n",
    "\n",
    "os.makedirs(distribution_folder, exist_ok=True)\n",
    "distribution_file_path = f\"{distribution_folder}/{dataset_prefix}.csv\"\n",
    "\n",
    "\n",
    "def get_json_files_by_folder(folder_path):\n",
    "    items = os.listdir(folder_path)\n",
    "    return [item for item in items if item.endswith('.json') and os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "json_files = get_json_files_by_folder(f\"{stats_folder_name}/{dataset_prefix}\")\n",
    "\n",
    "# Sort files prefix then index\n",
    "json_files.sort(key=lambda x: x.split(\"-\")[0])\n",
    "\n",
    "tasks_data = {}\n",
    "dataset_total = {}\n",
    "\n",
    "for file_name in json_files:\n",
    "    with open(f\"{stats_folder_name}/{dataset_prefix}/{file_name}\") as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "\n",
    "        dataset_total[file_name] = sum(data_dict[\"_task_name\"].values())\n",
    "\n",
    "        for task, count in data_dict[\"_task_name\"].items():\n",
    "            if task not in tasks_data:\n",
    "                tasks_data[task] = {}\n",
    "            tasks_data[task][file_name] = count\n",
    "\n",
    "\n",
    "header = [\"Task Name\"] + [file_name.split(\".\")[0] for file_name in json_files]\n",
    "\n",
    "with open(distribution_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for task, counts in tasks_data.items():\n",
    "        total_count = sum(counts.values())\n",
    "        row = [task] + [counts.get(file_name, 0) for file_name in json_files]\n",
    "        writer.writerow(row)\n",
    "\n",
    "    writer.writerow([\"Total\"] + [dataset_total[file_name] for file_name in json_files])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Diffs\n",
    "\n",
    "import difflib\n",
    "\n",
    "def compare_files(file1, file2):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        diff = difflib.unified_diff(\n",
    "            f1.readlines(),\n",
    "            f2.readlines(),\n",
    "            fromfile=file1,\n",
    "            tofile=file2,\n",
    "        )\n",
    "        return list(diff)\n",
    "\n",
    "def compare_multiple_files(file_list):\n",
    "    comparisons = {}\n",
    "    for i in range(len(file_list)):\n",
    "        for j in range(i + 1, len(file_list)):\n",
    "            file1 = file_list[i]\n",
    "            file2 = file_list[j]\n",
    "            diff = compare_files(file1, file2)\n",
    "            comparisons[f\"{file1} vs {file2}\"] = diff\n",
    "    return comparisons\n",
    "\n",
    "def save_diffs_to_html(diffs, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"<html><head><title>File Comparisons</title></head><body>\")\n",
    "        f.write(\"<h1>File Comparisons</h1>\")\n",
    "        for comparison, diff in diffs.items():\n",
    "            f.write(f\"<h2>Comparing {comparison}</h2>\")\n",
    "            f.write(\"<pre>\")\n",
    "            for line in diff:\n",
    "                if line.startswith('+'):\n",
    "                    f.write(f'<span style=\"color: green;\">{line}</span>')\n",
    "                elif line.startswith('-'):\n",
    "                    f.write(f'<span style=\"color: red;\">{line}</span>')\n",
    "                elif line.startswith('^'):\n",
    "                    f.write(f'<span style=\"color: blue;\">{line}</span>')\n",
    "                else:\n",
    "                    f.write(line)\n",
    "            f.write(\"</pre><hr>\")\n",
    "        f.write(\"</body></html>\")\n",
    "\n",
    "file_list = [f\"{stats_folder}/{file}\" for file in os.listdir(stats_folder_name) if file.endswith('.json')]\n",
    "diffs = compare_multiple_files(file_list)\n",
    "output_file = f\"comparison_output-{dataset_name}.html\"\n",
    "save_diffs_to_html(diffs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1611), 113577)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test values\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "file_path = f\"datasets/t0_fsnoopt_data/part.157.parquet\"\n",
    "df = load_dataset(file_path)\n",
    "\n",
    "# get count of task name \"wmt16_translate/cs-en:1.0.0\"\n",
    "task_name = \"wiki_bio_who\"\n",
    "task_count = df[\"_task_name\"].value_counts().get(task_name, 0)\n",
    "task_count, len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
